Speech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright © 2021. All
rights reserved. Draft of September 21, 2021.

CHAPTER
# 7 Neural Networks and Neural Language Models
7장 신경망과 신경 언어 모델

“[M]achines of this character can behave in a very complicated manner when the number of units is large.”
Alan Turing (1948) “Intelligent Machines”, page 6
 "단위의 수가 많으면 이러한 특성의 기계는 매우 복잡한 방식으로 행동할 수 있다." 앨런 튜링 (1948) "지능형 기계", 6페이지

Neural networks are a fundamental computational tool for language process-ing, and a very old one. They are called neural because their origins lie in the McCulloch-Pitts neuron (McCulloch and Pitts, 1943), a simplified model of the human neuron as a kind of computing element that could be described in terms of propositional logic. But the modern use in language processing no longer draws on these early biological inspirations.
신경망은 언어 처리를 위한 기본적인 계산 도구이며, 매우 오래된 도구입니다. 신경망이라는 이름은 논리 명제의 관점에서 설명될 수 있는 계산 요소의 일종으로서 인간 뉴런의 단순화된 모델인 맥컬록-피츠 뉴런(McCulloch and Pitts, 1943)에 기원을 두고 있기 때문에 붙여졌습니다. 하지만 언어 처리 분야에서 현대적인 사용은 더 이상 이러한 초기 생물학적 영감에 의존하지 않습니다.

Instead, a modern neural network is a network of small computing units, each of which takes a vector of input values and produces a single output value. In this chapter we introduce the neural net applied to classification. The architecture we introduce is called a feedforward network because the computation proceeds iter-feedforward
atively from one layer of units to the next. The use of modern neural nets is often called deep learning, because modern networks are often deep (have many layers).deep learning
대신, 현대 신경망은 작은 계산 단위들의 네트워크이며, 각 단위는 입력 값의 벡터를 받아 하나의 출력 값을 생성합니다. 이 장에서는 분류에 적용되는 신경망을 소개합니다. 여기서 소개하는 아키텍처는 피드포워드 네트워크라고 불리는데, 계산이 한 단위 계층에서 다음 계층으로 반복적으로 진행되기 때문입니다. 현대 신경망을 사용하는 것은 종종 딥 러닝이라고 불리는데, 현대 네트워크가 종종 깊기(많은 계층을 가지기) 때문입니다.

Neural networks share much of the same mathematics as logistic regression. But neural networks are a more powerful classifier than logistic regression, and indeed a minimal neural network (technically one with a single ‘hidden layer’) can be shown to learn any function.
Neural net classifiers are different from logistic regression in another way. With logistic regression, we applied the regression classifier to many different tasks by developing many rich kinds of feature templates based on domain knowledge. When working with neural networks, it is more common to avoid most uses of rich hand-derived features, instead building neural networks that take raw words as inputs and learn to induce features as part of the process of learning to classify. We saw examples of this kind of representation learning for embeddings in Chapter 6. Nets that are very deep are particularly good at representation learning. For that reason deep neural nets are the right tool for large scale problems that offer sufficient data to learn features automatically.
신경망은 로지스틱 회귀와 많은 수학적 내용을 공유합니다. 하지만 신경망은 로지스틱 회귀보다 더 강력한 분류기이며, 실제로 최소한의 신경망(기술적으로는 하나의 '은닉 계층'을 가진)은 어떤 함수라도 학습할 수 있는 것으로 보여집니다. 신경망 분류기는 로지스틱 회귀와 또 다른 면에서 다릅니다. 로지스틱 회귀에서는 도메인 지식에 기반한 풍부한 종류의 특징 템플릿을 개발하여 많은 다른 작업에 회귀 분류기를 적용했습니다. 신경망으로 작업할 때는 수작업으로 파생된 풍부한 특징의 대부분의 사용을 피하는 것이 더 일반적이며, 대신 원시 단어를 입력으로 받아 분류 학습 과정의 일부로 특징을 유도하도록 학습하는 신경망을 구축합니다. 우리는 6장에서 임베딩에 대한 이러한 종류의 표현 학습의 예시를 보았습니다. 매우 깊은 네트워크는 표현 학습에 특히 뛰어납니다. 그렇기 때문에 딥 신경망은 특징을 자동으로 학습하기 위한 충분한 데이터를 제공하는 대규모 문제에 적합한 도구입니다.

In this chapter we’ll introduce feedforward networks as classifiers, and also ap-ply them to the simple task of language modeling: assigning probabilities to word sequences and predicting upcoming words. In subsequent chapters we’ll introduce many other aspects of neural models, such as recurrent neural networks and the Transformer (Chapter 9), contextual embeddings like BERT (Chapter 11), and encoder-decoder models and attention (Chapter 10).
이 장에서는 피드포워드 네트워크를 분류기로 소개하고, 또한 언어 모델링이라는 간단한 작업(단어 시퀀스에 확률을 할당하고 다음 단어를 예측하는 것)에도 적용할 것입니다. 이후 장에서는 순환 신경망 및 트랜스포머(9장), BERT와 같은 문맥적 임베딩(11장), 인코더-디코더 모델 및 어텐션(10장)과 같은 신경 모델의 다른 많은 측면을 소개할 것입니다.
