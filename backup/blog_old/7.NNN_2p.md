**2 CHAPTER 7 • NEURAL NETWORKS AND NEURAL LANGUAGE MODELS**
**##### 7.1 Units**
The building block of a neural network is a **single computational unit**. A unit takes a set of real valued numbers as input, performs some computation on them, and produces an output.
At its heart, a neural unit is taking a **weighted sum of its inputs**, with one addi-tional term in the sum called a **bias term**. Given a set of inputs x1...xn, a unit hasbias term a set of corresponding weights w1...wn and a bias b, so the weighted sum z can be represented as:
$$z = b + \sum_{i} w_i x_i \quad (7.1)$$
Often it’s more convenient to express this weighted sum using **vector notation**; recall from linear algebra that a vector is, at heart, just a list or array of numbers. Thusvector we’ll talk about z in terms of a **weight vector w**, a **scalar bias b**, and an **input vector x**, and we’ll replace the sum with the convenient **dot product**:
$$z = \mathbf{w} \cdot \mathbf{x} + b \quad (7.2)$$
As defined in Eq. 7.2, z is just a **real valued number**. 

Finally, instead of using z, a linear function of x, as the output, neural units apply a **non-linear function f to z**. We will refer to the output of this function as the **activation value** for the unit, a. Since we are just modeling a single unit, theactivation activation for the node is in fact the **final output of the network**, which we’ll generally call **y**. So the value y is defined as:
$$y = a = f(z)$$
We’ll discuss three popular non-linear functions f () below (the sigmoid, the tanh, and the rectified linear ReLU) but it’s pedagogically convenient to start with the **sigmoid function** since we saw it in Chapter 5:
$$y = \sigma(z) = \frac{1}{1 + e^{-z}} \quad (7.3)$$
The sigmoid (shown in Fig. 7.1) has a number of advantages; it maps the output into the **range**, which is useful in squashing outliers toward 0 or 1. And it’s **differentiable**, which as we saw in Section ?? will be handy for learning.
Substituting Eq. 7.2 into Eq. 7.3 gives us the **output of a neural unit**:
$$y = \sigma(\mathbf{w} \cdot \mathbf{x} + b) = \frac{1}{1 + \exp(-(\mathbf{w} \cdot \mathbf{x} + b))} \quad (7.4)$$


Figure 7.1 The sigmoid function takes a real value and maps it to the range. It is nearly linear around 0 but outlier values get squashed toward 0 or 1.
